#PRATICAL:-1

Q.Write a Python program to plot a few activation functions that are being used in
 neural networks.

import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)

def softmax(x):
    e_x = np.exp(x - np.max(x))  # subtract max(x) for numerical stability
    return e_x / e_x.sum()

# Create x values
x = np.linspace(-10, 10, 100)

# Create plots for each activation function
fig, axs = plt.subplots(2, 2, figsize=(8, 8))
axs[0, 0].plot(x, sigmoid(x))
axs[0, 0].set_title('Sigmoid')
axs[0, 1].plot(x, relu(x))
axs[0, 1].set_title('ReLU')
axs[1, 0].plot(x, tanh(x))
axs[1, 0].set_title('Tanh')
axs[1, 1].plot(x, softmax(x))
axs[1, 1].set_title('Softmax')

# Add common axis labels and titles
fig.suptitle('Common Activation Functions')
for ax in axs.flat:
    ax.set(xlabel='x', ylabel='y')

# Adjust spacing between subplots
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4)

# Show the plot
plt.show()


******************************************************************************************************************************************
#Pratical-2

Q. Generate ANDNOT function using McCulloch-Pitts neural net by a python
 program

import numpy as np


# function of checking threshold value
def linear_threshold_gate(dot, T):
    '''Returns the binary threshold output'''
    if dot >= T:
        return 1
    else:
        return 0


# matrix of inputs
input_table = np.array([
    [0, 0],  # both no
    [0, 1],  # one no, one yes
    [1, 0],  # one yes, one no
    [1, 1]  # both yes
])

print(f'input table:\n{input_table}')

weights = np.array([1, -1])
dot_products = input_table @ weights
T = 1

for i in range(0, 4):
    activation = linear_threshold_gate(dot_products[i], T)
    print(f'Activation: {activation}')
******************************************************************************************************************************************
#Pratical-3

Q.Write a Python Program using Perceptron Neural Network to recognise even and
 odd numbers. Given numbers are in ASCII form 0 to 9

import numpy as np

# Define the perceptron class
class Perceptron:
    def __init__(self, input_size, lr=0.1):
        self.W = np.zeros(input_size + 1)
        self.lr = lr

    def activation_fn(self, x):
        return 1 if x >= 0 else 0

    def predict(self, x):
        x = np.insert(x, 0, 1)
        z = self.W.T.dot(x)
        a = self.activation_fn(z)
        return a

    def train(self, X, Y, epochs):
        for _ in range(epochs):
            for i in range(Y.shape[0]):
                x = X[i]
                y = self.predict(x)
                e = Y[i] - y
                self.W = self.W + self.lr * e * np.insert(x, 0, 1)

# Define the input data and labels
X = np.array([
    [0,0,0,0,0,0,1,0,0,0], # 0
    [0,0,0,0,0,0,0,1,0,0], # 1
    [0,0,0,0,0,0,0,0,1,0], # 2
    [0,0,0,0,0,0,0,0,0,1], # 3
    [0,0,0,0,0,0,1,1,0,0], # 4
    [0,0,0,0,0,0,1,0,1,0], # 5
    [0,0,0,0,0,0,1,1,1,0], # 6
    [0,0,0,0,0,0,1,1,1,1], # 7
    [0,0,0,0,0,0,1,0,1,1], # 8
    [0,0,0,0,0,0,0,1,1,1], # 9
])
Y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])

# Create the perceptron and train it
perceptron = Perceptron(input_size=10)
perceptron.train(X, Y, epochs=100)

# Test the perceptron on some input data
test_X = np.array([
    [0,0,0,0,0,0,1,0,0,0], # 0
    [0,0,0,0,0,0,0,1,0,0], # 1
    [0,0,0,0,0,0,0,0,1,0], # 2
    [0,0,0,0,0,0,0,0,0,1], # 3
    [0,0,0,0,0,0,1,1,0,0], # 4
    [0,0,0,0,0,0,1,0,1,0], # 5
    [0,0,0,0,0,0,1,1,1,0], # 6
    [0,0,0,0,0,0,1,1,1,1], # 7
    [0,0,0,0,0,0,1,0,1,1], # 8
    [0,0,0,0,0,0,0,1,1,1], # 9
])

for i in range(test_X.shape[0]):
    x = test_X[i]
    y = perceptron.predict(x)
    print(f'{x} is {"even" if y == 0 else "odd"}')

*****************************************************************************************************************************************

PRATICAL4

Q. With a suitable example demonstrate the perceptron learning law with its decision regions using
 python. Give the output in graphical form.



import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

# Load iris dataset
iris = load_iris()

# Extract sepal length and petal length features
X = iris.data[:, [0, 2]]
y = iris.target

# Setosa is class 0, versicolor is class 1
y = np.where(y == 0, 1, 0)

# Initialize weights and bias
w = np.zeros(2)
b = 0

# Set learning rate and number of epochs
lr = 0.1
epochs = 50

# Define perceptron function
def perceptron(x, w, b):
    # Calculate weighted sum of inputs
    z = np.dot(x, w) + b
    # Apply step function
    return np.where(z >= 0, 1, 0)

# Train the perceptron
for epoch in range(epochs):
    for i in range(len(X)):
        x = X[i]
        target = y[i]
        output = perceptron(x, w, b)
        error = target - output
        w += lr * error * x
        b += lr * error

# Plot decision boundary
x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
Z = perceptron(np.c_[xx.ravel(), yy.ravel()], w, b)
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)

# Plot data points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Petal length')
plt.title('Perceptron decision regions')
plt.show()


******************************************************************************************************************************************

PRATICAL 5

Q. Implement Artificial Neural Network training process in Python by using Forward 
Propagation, Back Propagation.
import numpy as np

# Define sigmoid activation function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Define derivative of sigmoid activation function
def sigmoid_derivative(x):
    return x * (1 - x)

# Define training inputs
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])

# Define training outputs
outputs = np.array([[0], [1], [1], [0]])

# Define random weights for hidden layer
hidden_weights = np.random.uniform(size=(2, 3))

# Define random weights for output layer
output_weights = np.random.uniform(size=(3, 1))

# Set learning rate
learning_rate = 0.7

# Train the network
for i in range(10000):

    # Forward propagation
    hidden_layer_activation = sigmoid(np.dot(inputs, hidden_weights))
    output_layer_activation = sigmoid(np.dot(hidden_layer_activation, output_weights))

    # Calculate error
    error = outputs - output_layer_activation

    # Backpropagation
    output_layer_error = error * sigmoid_derivative(output_layer_activation)
    hidden_layer_error = np.dot(output_layer_error, output_weights.T) * sigmoid_derivative(hidden_layer_activation)

    # Update weights
    output_weights += np.dot(hidden_layer_activation.T, output_layer_error) * learning_rate
    hidden_weights += np.dot(inputs.T, hidden_layer_error) * learning_rate

# Test the network
hidden_layer_activation = sigmoid(np.dot(inputs, hidden_weights))
output_layer_activation = sigmoid(np.dot(hidden_layer_activation, output_weights))
print(output_layer_activation)

*****************************************************************************************************************************************


Pratical 6

Q.  Create a Neural network architecture from scratch in Python and use it to do multi-class classification on 
any data. 

WRITE IN IPYNB FILE

import numpy as np
from sklearn.preprocessing import OneHotEncoder


class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.input_size = input_size
        self.hidden_layers = hidden_layers
        self.output_size = output_size
        self.weights = []
        self.biases = []
        self.activations = []

        # Initialize weights and biases for hidden layers
        for i in range(len(hidden_layers)):
            if i == 0:
                self.weights.append(np.random.randn(input_size, hidden_layers[i]))
            else:
                self.weights.append(np.random.randn(hidden_layers[i-1], hidden_layers[i]))
            self.biases.append(np.zeros((1, hidden_layers[i])))
            self.activations.append(self.relu)

        # Initialize weights and biases for output layer
        self.weights.append(np.random.randn(hidden_layers[-1], output_size))
        self.biases.append(np.zeros((1, output_size)))
        self.activations.append(self.softmax)

    def relu(self, x):
        return np.maximum(0, x)

    def softmax(self, x):
        exps = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exps / np.sum(exps, axis=1, keepdims=True)

    def forward_pass(self, x):
        activations = [x]
        for i in range(len(self.weights)):
            x = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            x = self.activations[i](x)
            activations.append(x)
        return activations

    def compute_loss(self, y_true, y_pred):
        m = y_true.shape[0]
        log_likelihood = -np.log(y_pred[range(m), np.argmax(y_true, axis=1)])
        loss = np.sum(log_likelihood) / m
        return loss

    def backward_pass(self, x, y_true, activations):
        m = y_true.shape[0]
        grads_weights = [np.zeros_like(w) for w in self.weights]
        grads_biases = [np.zeros_like(b) for b in self.biases]

        error = y_true - activations[-1]
        grads_weights[-1] = np.dot(activations[-2].T, error) / m
        grads_biases[-1] = np.sum(error, axis=0, keepdims=True) / m

        for i in range(len(self.weights)-2, -1, -1):
            error = np.dot(error, self.weights[i+1].T) * (activations[i+1] > 0)
            grads_weights[i] = np.dot(activations[i].T, error) / m
            grads_biases[i] = np.sum(error, axis=0, keepdims=True) / m

        return grads_weights, grads_biases

    def update_params(self, grads_weights, grads_biases, learning_rate):
        for i in range(len(self.weights)):
            self.weights[i] += learning_rate * grads_weights[i]
            self.biases[i] += learning_rate * grads_biases[i]

    def train(self, X_train, y_train, epochs, learning_rate):
        for epoch in range(epochs):
            activations = self.forward_pass(X_train)
            loss = self.compute_loss(y_train, activations[-1])
            grads_weights, grads_biases = self.backward_pass(X_train, y_train, activations)
            self.update_params(grads_weights, grads_biases, learning_rate)
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss}")

    def predict(self, X):
        activations = self.forward_pass(X)
        return np.argmax(activations[-1], axis=1)




# Dummy data
X_train = np.random.randn(1000, 10)  # 1000 samples, 10 features
y_train = np.random.randint(0, 3, size=(1000,))  # 3 classes

# One-hot encode the labels
encoder = OneHotEncoder(sparse=False)
y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))


# Create neural network
input_size = X_train.shape[1]
hidden_layers = [100]
output_size = len(np.unique(y_train))
nn = NeuralNetwork(input_size, hidden_layers, output_size)

# Train neural network
nn.train(X_train, y_train_encoded, epochs=1000, learning_rate=0.01)

# Predictions
predictions = nn.predict(X_train)
*****************************************************************************************************************************************

pratical 7

Q.        Write a python program to illustrate ART neural network

!pip install adversarial-robustness-toolbox


import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import FastGradientMethod

# Generate some synthetic data for demonstration
X_train = np.random.rand(1000, 10).astype(np.float32)
y_train = np.random.randint(0, 2, size=(1000,)).astype(np.float32)

# Define a simple neural network model using TensorFlow/Keras
model = models.Sequential([
    layers.Dense(128, activation='relu', input_shape=(10,)),
    layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32)

# Create an ART classifier from the trained model
art_classifier = TensorFlowV2Classifier(model=model, nb_classes=2, input_shape=(10,),
                                        loss_object=tf.keras.losses.BinaryCrossentropy(),
                                        clip_values=(0, 1))

# Define an adversarial attack
attack = FastGradientMethod(estimator=art_classifier, eps=0.1)

# Generate adversarial examples
X_test_adv = attack.generate(x=X_train)

# Evaluate the robustness of the classifier
accuracy_adv = np.sum((art_classifier.predict(X_test_adv)[:, 0] >= 0.5).astype(int) == y_train) / len(y_train)

print("Accuracy on adversarial examples:", accuracy_adv)

*******************************************************************************************************************

PRATICAL no.8
Q.Write a python program for creating a Back Propagation Feed-forward neural network


import numpy as  np

# define sigmoid activation function
def sigmoid(x):
 return 1 / (1 + np.exp(-x))
# Define derivative of sigmoid function
def sigmoid_derivative(x):
 return x * (1 - x)
# Define input dataset
X = np.array([[0,0], [0,1], [1,0], [1,1]])
# Define output dataset
y = np.array([[0], [1], [1], [0]])
# Define hyperparameters
learning_rate = 0.1
num_epochs = 100000
# Initialize weights randomly with mean 0
hidden_weights = 2*np.random.random((2,2)) - 1
output_weights = 2*np.random.random((2,1)) - 1
# Train the neural network
for i in range(num_epochs):
 # Forward propagation
 hidden_layer = sigmoid(np.dot(X, hidden_weights))
 output_layer = sigmoid(np.dot(hidden_layer, output_weights))
 # Backpropagation
 output_error = y - output_layer
 output_delta = output_error * sigmoid_derivative(output_layer)
 hidden_error = output_delta.dot(output_weights.T)
 hidden_delta = hidden_error * sigmoid_derivative(hidden_layer)
 output_weights += hidden_layer.T.dot(output_delta) * learning_rate
 hidden_weights += X.T.dot(hidden_delta) * learning_rate
# Display input and output
print("Input:")
print(X)
print("Output:")
print(output_layer)


*******************************************************************************************************************
PRATICAL no 9

Q Write a python program to design a Hopfield Network which stores 4 vectors



import numpy as np

class HopfieldNetwork:
    def __init__(self, pattern_size):
        self.pattern_size = pattern_size
        self.weights = np.zeros((pattern_size, pattern_size))

    def train(self, patterns):
        for pattern in patterns:
            self.weights += np.outer(pattern, pattern)
        np.fill_diagonal(self.weights, 0)

    def predict(self, pattern, max_iter=100):
        old_pattern = pattern.copy()
        for _ in range(max_iter):
            new_pattern = np.sign(np.dot(self.weights, old_pattern))
            if np.array_equal(new_pattern, old_pattern):
                return new_pattern
            old_pattern = new_pattern
        return old_pattern

# Example usage
if __name__ == "__main__":
    patterns = np.array([[1, -1, 1, -1],
                         [-1, 1, -1, 1],
                         [1, 1, 1, 1]])

    hopfield_net = HopfieldNetwork(pattern_size=4)
    hopfield_net.train(patterns)

    test_pattern = np.array([-1, 1, 1, 1])
    predicted_pattern = hopfield_net.predict(test_pattern)
    print("Predicted Pattern:", predicted_pattern)



OR
 

import numpy as np

class HopfieldNetwork:
    def __init__(self, pattern_size):
        self.pattern_size = pattern_size
        self.weights = np.zeros((pattern_size, pattern_size))

    def train(self, patterns):
        for pattern in patterns:
            self.weights += np.outer(pattern, pattern)
        np.fill_diagonal(self.weights, 0)

    def predict(self, pattern, max_iter=100):
        old_pattern = pattern.copy()
        for _ in range(max_iter):
            new_pattern = np.sign(np.dot(self.weights, old_pattern))
            if np.array_equal(new_pattern, old_pattern):
                return new_pattern
            old_pattern = new_pattern
        return old_pattern

# Example usage
if __name__ == "__main__":
    patterns = np.array([[1, -1, 1, -1],
                         [-1, 1, -1, 1],
                         [1, 1, 1, 1],
                         [-1, -1, -1, -1]])

    hopfield_net = HopfieldNetwork(pattern_size=4)
    hopfield_net.train(patterns)

    test_pattern = np.array([-1, 1, -1, 1])
    predicted_pattern = hopfield_net.predict(test_pattern)
    print("Predicted Pattern:", predicted_pattern)


*******************************************************************************************************************


EXPERIMENT:-10 ( tensor flow error)

Q. How to Train a Neural Network with TensorFlow/Pytorch and evaluation of logistic regression using tensorflow.

import keras
from keras.datasets import cifar10
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import to_categorical

# Load CIFAR-10 dataset
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# Convert labels to categorical
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Define the model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

# Define data generators
train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
test_datagen = ImageDataGenerator(rescale=1./255)

# Prepare the data
X_train = train_datagen.standardize(X_train)
X_test = test_datagen.standardize(X_test)

# Compile the model
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))

# Evaluate the model
score = model.evaluate(X_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])


*******************************************************************************************************************

Epperiment 11

Q.TensorFlow/Pytorch implementation of CNN.




PROGRM 1

import torch
import torch.nn as nn
import torch.nn.functional as F

# Define the CNN architecture
class CNN(nn.Module):
    def __init__(self, input_channels, num_classes):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 7 * 7, 64)
        self.fc2 = nn.Linear(64, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, kernel_size=2, stride=2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, kernel_size=2, stride=2)
        x = F.relu(self.conv3(x))
        x = F.max_pool2d(x, kernel_size=2, stride=2)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

# Example usage
input_channels = 1  # Example input channels for MNIST images
num_classes = 10  # Example number of classes for MNIST dataset

model = CNN(input_channels, num_classes)
print(model)

PROGRAM 2

import tensorflow as tf
import matplotlib.pyplot as plt

# Define a simple CNN model using TensorFlow/Keras
def create_tf_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu')
    ])
    return model

# Create an example input image
input_image = tf.random.uniform((1, 28, 28, 1))

# Create and compile the model
tf_model = create_tf_model()
tf_model.compile(optimizer='adam', loss='mse')

# Get the output from the model
output_image = tf_model.predict(input_image)

# Plot the input and output images
plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.imshow(input_image[0, :, :, 0], cmap='gray')
plt.title('Input Image')
plt.subplot(1, 2, 2)
plt.imshow(output_image[0, :, :, 0], cmap='gray')
plt.title('Output Image')
plt.show()

*******************************************************************************************************************


pratical no 12:- 

Q. MNIST Handwritten Character Detection using PyTorch, Keras and TensorFlow


program 1
import tensorflow as tf
from tensorflow.keras import layers, models

# Load and prepare MNIST dataset
mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

# Define the model architecture
model = models.Sequential([
    layers.Flatten(input_shape=(28, 28)),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(train_images, train_labels, epochs=5)

# Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f'Test accuracy: {test_acc}')



 OR


program2:-
import tensorflow as tf
import matplotlib.pyplot as plt

# Load pre-trained model
model = tf.keras.applications.ResNet50(include_top=True, weights='imagenet')

# Load MNIST data
mnist_data = tf.keras.datasets.mnist
(_, _), (x_test, y_test) = mnist_data.load_data()
x_test = tf.image.grayscale_to_rgb(tf.expand_dims(x_test, axis=-1))

# Choose a random image from MNIST dataset
image = x_test[1]

# Resize the image to match the input shape of the ResNet50 model
image_resized = tf.image.resize(image, (224, 224))

# Make prediction
output = model.predict(image_resized[np.newaxis, ...])

# Get predicted label
predicted = tf.argmax(output, axis=-1)

# Display image and predicted label
plt.imshow(image.numpy().squeeze(), cmap='gray')
plt.title(f'Predicted Label: {predicted.item()}, Actual Label: {y_test[0]}')
plt.axis('off')
plt.show()






